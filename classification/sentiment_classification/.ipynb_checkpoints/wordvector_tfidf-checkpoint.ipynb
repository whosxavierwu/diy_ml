{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost \n",
    "import lightgbm\n",
    "\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "from pyhanlp import *\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors, TfidfModel\n",
    "from gensim.similarities import SparseMatrixSimilarity\n",
    "\n",
    "from WordVectorFetcher import WordVectorFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_id</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>[img]http://img.autohome.com.cn/album/smiles/s...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>“戏说”奔驰女再次向奔驰维权：要求赔偿240万--致广大网友的一封公开信广大支持过我的网友，...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>“这辆二手车多少钱买的?”因为家门口修车店维修工的这一句话，车主殷小姐憋了一肚子气，开着新买...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            content  content_id  \\\n",
       "0  negative  [img]http://img.autohome.com.cn/album/smiles/s...           0   \n",
       "1  negative  “戏说”奔驰女再次向奔驰维权：要求赔偿240万--致广大网友的一封公开信广大支持过我的网友，...           1   \n",
       "2  negative  “这辆二手车多少钱买的?”因为家门口修车店维修工的这一句话，车主殷小姐憋了一肚子气，开着新买...           2   \n",
       "\n",
       "   label_id  \n",
       "0        -1  \n",
       "1        -1  \n",
       "2        -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load data \n",
    "df_data = pd.read_csv('data/sentiment_corpus_20191108.txt', encoding='utf8', sep='\\t', names=['label', 'content'])\n",
    "label2id = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "df_data['content_id'] = range(len(df_data))\n",
    "df_data['label_id'] = df_data['label'].apply(lambda x: label2id[x])\n",
    "print(df_data.shape)\n",
    "df_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLPTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "def seg(doc):\n",
    "    tokens = []\n",
    "#     for item in NLPTokenizer.segment(doc):\n",
    "    for item in HanLP.segment(doc):\n",
    "        word = item.word\n",
    "        tag = item.nature.toString()\n",
    "        # http://www.hankcs.com/nlp/part-of-speech-tagging.html#h2-8\n",
    "        if tag[0] not in ['b','m','p','q','u','x']:\n",
    "            tokens.append(word)\n",
    "#         tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vector file...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "class TfidfWordVectorCombiner:\n",
    "    def __init__(self):\n",
    "        self.dictionary = None\n",
    "        self.tfidf_model = None\n",
    "        self.fetcher = WordVectorFetcher('tmp/sgns.sogou.word.bz2')\n",
    "#         self.fetcher = WordVectorFetcher('tmp/sgns.zhihu.bigram-char.bz2')\n",
    "        print('Loading word vector file...')\n",
    "        self.fetcher.init()\n",
    "        print('Done')\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        corpus_train = list(df_train['content'].apply(seg))\n",
    "        self.dictionary = gensim.corpora.Dictionary(corpus_train)\n",
    "        with codecs.open('tmp/tfidf_vocab.csv', 'w', encoding='utf8') as fout:\n",
    "            for id, tok in self.dictionary.id2token.items():\n",
    "                fout.write(\"{}\\t{}\\n\".format(id, tok))\n",
    "        corpus_train_bow = [self.dictionary.doc2bow(tokens) for tokens in corpus_train]\n",
    "        self.tfidf_model = TfidfModel(corpus_train_bow)\n",
    "        return\n",
    "\n",
    "    def transform(self, df):\n",
    "        corpus = list(df['content'].apply(seg))\n",
    "        corpus_bow = [self.dictionary.doc2bow(tokens) for tokens in corpus]\n",
    "        tfidf_corpus = [t for t in self.tfidf_model[corpus_bow]]\n",
    "        arr = []\n",
    "        for tfidf_doc in tfidf_corpus:\n",
    "            vec = np.zeros_like(self.fetcher.get_word_vector(u\"\"))\n",
    "            for token_id, token_tfidf in tfidf_doc:\n",
    "                token = self.dictionary[token_id]\n",
    "                vec += token_tfidf * self.fetcher.get_word_vector(token)\n",
    "            arr.append(vec.reshape((1, len(vec))))\n",
    "        X = np.concatenate(arr)\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, df_train):\n",
    "        corpus_train = list(df_train['content'].apply(seg))\n",
    "        self.dictionary = gensim.corpora.Dictionary(corpus_train)\n",
    "        with codecs.open('tmp/tfidf_vocab.csv', 'w', encoding='utf8') as fout:\n",
    "            for id, tok in self.dictionary.id2token.items():\n",
    "                fout.write(\"{}\\t{}\\n\".format(id, tok))\n",
    "        corpus_train_bow = [self.dictionary.doc2bow(tokens) for tokens in corpus_train]\n",
    "        self.tfidf_model = TfidfModel(corpus_train_bow)\n",
    "        \n",
    "        tfidf_corpus = [t for t in self.tfidf_model[corpus_train_bow]]\n",
    "        arr = []\n",
    "        for tfidf_doc in tfidf_corpus:\n",
    "            vec = np.zeros_like(self.fetcher.get_word_vector(u\"\"))\n",
    "            for token_id, token_tfidf in tfidf_doc:\n",
    "                token = self.dictionary[token_id]\n",
    "                vec += token_tfidf * self.fetcher.get_word_vector(token)\n",
    "            arr.append(vec.reshape((1, len(vec))))\n",
    "        X = np.concatenate(arr)\n",
    "        return X\n",
    "\n",
    "combiner = TfidfWordVectorCombiner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleModel():\n",
    "    def __init__(self):\n",
    "        self.xgb = xgboost.XGBClassifier(\n",
    "            n_estimators=100, \n",
    "            n_jobs=-1, \n",
    "            objective='multi:softmax', \n",
    "            num_class=3,\n",
    "            max_depth=3,\n",
    "            subsample=0.8,\n",
    "            gamma=0\n",
    "        )\n",
    "        self.lr = LogisticRegression(\n",
    "            n_jobs=-1, \n",
    "            solver='lbfgs', \n",
    "            multi_class='auto',\n",
    "            max_iter=500\n",
    "        )\n",
    "        self.svc = SVC(\n",
    "            gamma='scale', \n",
    "            kernel='rbf'\n",
    "        )\n",
    "        self.knn = KNeighborsClassifier(\n",
    "            n_neighbors=5, \n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        print('### fitting xgb...')\n",
    "        self.xgb.fit(X_train, y_train)\n",
    "        print('### fitting svc...')\n",
    "        self.svc.fit(X_train, y_train)\n",
    "        print('### fitting knn...')\n",
    "        self.knn.fit(X_train, y_train)\n",
    "        print('### fitting lr...')\n",
    "        self.lr.fit(X_train, y_train)\n",
    "        \n",
    "    def predict(self, X_val, y_val=None):\n",
    "        n = len(X_val)\n",
    "        y_xgb = self.xgb.predict(X_val)\n",
    "        y_svc = self.svc.predict(X_val)\n",
    "        y_knn = self.knn.predict(X_val)\n",
    "        y_lr = self.lr.predict(X_val)\n",
    "        y_val_pred = np.concatenate([\n",
    "            y_xgb.reshape((n, 1)), \n",
    "            y_svc.reshape((n, 1)),\n",
    "            y_knn.reshape((n, 1)),\n",
    "            y_lr.reshape((n, 1)),\n",
    "        ], axis=1)\n",
    "        if y_val is not None:\n",
    "            print(\n",
    "                metrics.accuracy_score(y_true=y_val, y_pred=y_xgb),\n",
    "                metrics.accuracy_score(y_true=y_val, y_pred=y_svc),\n",
    "                metrics.accuracy_score(y_true=y_val, y_pred=y_knn),\n",
    "                metrics.accuracy_score(y_true=y_val, y_pred=y_lr),\n",
    "            )\n",
    "        y_val_pred = [Counter(i).most_common(1)[0][0] for i in y_val_pred]\n",
    "        return y_val_pred\n",
    "\n",
    "model = EnsembleModel()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Spliting data...\n",
      "### Preparing features...\n",
      "### fitting xgb...\n",
      "### fitting svc...\n",
      "### fitting knn...\n",
      "### fitting lr...\n",
      "0.9516666666666667 0.8441666666666666 0.8433333333333334 0.8833333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95       800\n",
      "     neutral       0.93      0.86      0.89       800\n",
      "    positive       0.93      0.95      0.94       800\n",
      "\n",
      "    accuracy                           0.93      2400\n",
      "   macro avg       0.93      0.93      0.93      2400\n",
      "weighted avg       0.93      0.93      0.93      2400\n",
      "\n",
      "0.8066666666666666 0.82 0.7733333333333333 0.7783333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87       200\n",
      "     neutral       0.75      0.70      0.72       200\n",
      "    positive       0.82      0.86      0.84       200\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.81      0.81      0.81       600\n",
      "weighted avg       0.81      0.81      0.81       600\n",
      "\n",
      "### Spliting data...\n",
      "### Preparing features...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7db0873b028a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'### Preparing features...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombiner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombiner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-cb4814d8f58c>\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, df_train)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mcorpus_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tmp/tfidf_vocab.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda2\\envs\\tf2\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4040\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4042\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4044\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0acc40f0747f>\u001b[0m in \u001b[0;36mseg\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     for item in NLPTokenizer.segment(doc):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mHanLP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msegment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda2\\envs\\tf2\\lib\\site-packages\\jpype\\_jclass.py\u001b[0m in \u001b[0;36m_JClassNew\u001b[1;34m(arg, loader, initialize)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_JClassNew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mloader\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0marg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_java_lang_Class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print('### Spliting data...')\n",
    "    df_train, df_val = model_selection.train_test_split(\n",
    "        df_data, test_size=0.2, \n",
    "    #     random_state=42, \n",
    "        shuffle=True, \n",
    "        stratify=df_data['label']\n",
    "    )\n",
    "    y_train = df_train['label'].values\n",
    "    y_val = df_val['label'].values\n",
    "\n",
    "    print('### Preparing features...')\n",
    "    X_train = combiner.fit_transform(df_train)\n",
    "    X_val = combiner.transform(df_val)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(metrics.classification_report(y_true=y_train, y_pred=model.predict(X_train, y_train)))\n",
    "    print(metrics.classification_report(y_true=y_val, y_pred=model.predict(X_val, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_data, y_data)\n",
    "# y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv('data/real_senti_demo_nolabel.txt', encoding='utf8', sep='\\t', names=['content'])\n",
    "# X_test = combiner.transform(df_test)\n",
    "# y_test_pred = model.predict(X_test)\n",
    "# df_test['label'] = y_test_pred\n",
    "# df_test[['label', 'content']].to_csv('data/submission.csv', encoding='utf8', sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# params_for_svc = [\n",
    "#     {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
    "#     {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}\n",
    "# ]\n",
    "# cv_params = {'n_estimators': [400, 500, 600, 700, 800]}\n",
    "# other_params = {\n",
    "#     'learning_rate': 0.1, \n",
    "#     'n_estimators': 500, \n",
    "#     'max_depth': 5, \n",
    "#     'min_child_weight': 1, \n",
    "#     'seed': 0,\n",
    "#     'subsample': 0.8, \n",
    "#     'colsample_bytree': 0.8, \n",
    "#     'gamma': 0, \n",
    "#     'reg_alpha': 0, \n",
    "#     'reg_lambda': 1, \n",
    "#     'n_jobs': -1\n",
    "# }\n",
    "# model = xgboost.XGBClassifier(**other_params)\n",
    "# optimized_GBM = model_selection.GridSearchCV(\n",
    "#     estimator=model, param_grid=cv_params, \n",
    "#     scoring='accuracy', cv=5, verbose=True, n_jobs=-1\n",
    "# )\n",
    "# optimized_GBM.fit(X_train, y_train)\n",
    "# evalute_result = optimized_GBM.grid_scores_\n",
    "# print('每轮迭代运行结果:{0}'.format(evalute_result))\n",
    "# print('参数的最佳取值：{0}'.format(optimized_GBM.best_params_))\n",
    "# print('最佳模型得分:{0}'.format(optimized_GBM.best_score_))\n",
    "\n",
    "# y_true, y_pred = y_val, clf.predict(X_val)\n",
    "# print(metrics.classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
