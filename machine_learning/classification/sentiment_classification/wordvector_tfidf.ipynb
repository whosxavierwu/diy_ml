{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import xgboost \n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "from pyhanlp import *\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors, TfidfModel\n",
    "from gensim.similarities import SparseMatrixSimilarity\n",
    "\n",
    "from WordVectorFetcher import WordVectorFetcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_id</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>[img]http://img.autohome.com.cn/album/smiles/s...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>“戏说”奔驰女再次向奔驰维权：要求赔偿240万--致广大网友的一封公开信广大支持过我的网友，...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>“这辆二手车多少钱买的?”因为家门口修车店维修工的这一句话，车主殷小姐憋了一肚子气，开着新买...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            content  content_id  \\\n",
       "0  negative  [img]http://img.autohome.com.cn/album/smiles/s...           0   \n",
       "1  negative  “戏说”奔驰女再次向奔驰维权：要求赔偿240万--致广大网友的一封公开信广大支持过我的网友，...           1   \n",
       "2  negative  “这辆二手车多少钱买的?”因为家门口修车店维修工的这一句话，车主殷小姐憋了一肚子气，开着新买...           2   \n",
       "\n",
       "   label_id  \n",
       "0        -1  \n",
       "1        -1  \n",
       "2        -1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load data \n",
    "df_data = pd.read_csv('data/sentiment_corpus_20191108.txt', encoding='utf8', sep='\\t', names=['label', 'content'])\n",
    "label2id = {'negative': -1, 'neutral': 0, 'positive': 1}\n",
    "df_data['content_id'] = range(len(df_data))\n",
    "df_data['label_id'] = df_data['label'].apply(lambda x: label2id[x])\n",
    "print(df_data.shape)\n",
    "df_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLPTokenizer = JClass(\"com.hankcs.hanlp.tokenizer.NLPTokenizer\")\n",
    "def seg(doc):\n",
    "    tokens = []\n",
    "#     for item in NLPTokenizer.segment(doc):\n",
    "    for item in HanLP.segment(doc):\n",
    "        word = item.word\n",
    "        tag = item.nature.toString()\n",
    "        # http://www.hankcs.com/nlp/part-of-speech-tagging.html#h2-8\n",
    "        if tag[0] not in ['b','m','p','q','u','x']:\n",
    "            tokens.append(word)\n",
    "#         tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vector file...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "class TfidfWordVectorCombiner:\n",
    "    def __init__(self):\n",
    "        self.dictionary = None\n",
    "        self.tfidf_model = None\n",
    "        self.fetcher = WordVectorFetcher('tmp/sgns.sogou.word.bz2')\n",
    "#         self.fetcher = WordVectorFetcher('tmp/sgns.zhihu.bigram-char.bz2')\n",
    "        print('Loading word vector file...')\n",
    "        self.fetcher.init()\n",
    "        print('Done')\n",
    "    \n",
    "    def fit(self, df_train):\n",
    "        corpus_train = list(df_train['content'].apply(seg))\n",
    "        self.dictionary = gensim.corpora.Dictionary(corpus_train)\n",
    "        with codecs.open('tmp/tfidf_vocab.csv', 'w', encoding='utf8') as fout:\n",
    "            for id, tok in self.dictionary.id2token.items():\n",
    "                fout.write(\"{}\\t{}\\n\".format(id, tok))\n",
    "        corpus_train_bow = [self.dictionary.doc2bow(tokens) for tokens in corpus_train]\n",
    "        self.tfidf_model = TfidfModel(corpus_train_bow)\n",
    "        return\n",
    "\n",
    "    def transform(self, df):\n",
    "        corpus = list(df['content'].apply(seg))\n",
    "        corpus_bow = [self.dictionary.doc2bow(tokens) for tokens in corpus]\n",
    "        tfidf_corpus = [t for t in self.tfidf_model[corpus_bow]]\n",
    "        arr = []\n",
    "        for tfidf_doc in tfidf_corpus:\n",
    "            vec = np.zeros_like(self.fetcher.get_word_vector(u\"\"))\n",
    "            for token_id, token_tfidf in tfidf_doc:\n",
    "                token = self.dictionary[token_id]\n",
    "                vec += token_tfidf * self.fetcher.get_word_vector(token)\n",
    "            arr.append(vec.reshape((1, len(vec))))\n",
    "        X = np.concatenate(arr)\n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, df_train):\n",
    "        corpus_train = list(df_train['content'].apply(seg))\n",
    "        self.dictionary = gensim.corpora.Dictionary(corpus_train)\n",
    "        with codecs.open('tmp/tfidf_vocab.csv', 'w', encoding='utf8') as fout:\n",
    "            for id, tok in self.dictionary.id2token.items():\n",
    "                fout.write(\"{}\\t{}\\n\".format(id, tok))\n",
    "        corpus_train_bow = [self.dictionary.doc2bow(tokens) for tokens in corpus_train]\n",
    "        self.tfidf_model = TfidfModel(corpus_train_bow)\n",
    "        \n",
    "        tfidf_corpus = [t for t in self.tfidf_model[corpus_train_bow]]\n",
    "        arr = []\n",
    "        for tfidf_doc in tfidf_corpus:\n",
    "            vec = np.zeros_like(self.fetcher.get_word_vector(u\"\"))\n",
    "            for token_id, token_tfidf in tfidf_doc:\n",
    "                token = self.dictionary[token_id]\n",
    "                vec += token_tfidf * self.fetcher.get_word_vector(token)\n",
    "            arr.append(vec.reshape((1, len(vec))))\n",
    "        X = np.concatenate(arr)\n",
    "        return X\n",
    "\n",
    "combiner = TfidfWordVectorCombiner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('tmp/tfidf_vocab.csv', 'w', encoding='utf8') as fout:\n",
    "    for id, tok in combiner.dictionary.id2token.items():\n",
    "        fout.write(\"{}\\t{}\\n\".format(id, tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=500, n_jobs=-1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n"
     ]
    }
   ],
   "source": [
    "xgb = xgboost.XGBClassifier(n_estimators=500, n_jobs=-1)\n",
    "rf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "lr = LogisticRegression(n_jobs=-1, solver='lbfgs', multi_class='auto')\n",
    "svc = SVC(gamma='scale', kernel='rbf')\n",
    "\n",
    "model = xgb\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Spliting data...\n",
      "y_train: (2400,) y_val: (600,)\n",
      "### Preparing features...\n",
      "X_train: (2400, 300) X_val: (600, 300)\n",
      "### fitting...\n",
      " Classification Report on Train set \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       787\n",
      "           0       1.00      1.00      1.00       817\n",
      "           1       1.00      1.00      1.00       796\n",
      "\n",
      "    accuracy                           1.00      2400\n",
      "   macro avg       1.00      1.00      1.00      2400\n",
      "weighted avg       1.00      1.00      1.00      2400\n",
      "\n",
      " Classification Report on Val set \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.88      0.85       213\n",
      "           0       0.72      0.68      0.70       183\n",
      "           1       0.87      0.86      0.87       204\n",
      "\n",
      "    accuracy                           0.81       600\n",
      "   macro avg       0.81      0.81      0.81       600\n",
      "weighted avg       0.81      0.81      0.81       600\n",
      "\n",
      "### Spliting data...\n",
      "y_train: (2400,) y_val: (600,)\n",
      "### Preparing features...\n",
      "X_train: (2400, 300) X_val: (600, 300)\n",
      "### fitting...\n",
      " Classification Report on Train set \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       806\n",
      "           0       1.00      1.00      1.00       799\n",
      "           1       1.00      1.00      1.00       795\n",
      "\n",
      "    accuracy                           1.00      2400\n",
      "   macro avg       1.00      1.00      1.00      2400\n",
      "weighted avg       1.00      1.00      1.00      2400\n",
      "\n",
      " Classification Report on Val set \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.83      0.91      0.87       194\n",
      "           0       0.75      0.62      0.68       201\n",
      "           1       0.81      0.87      0.84       205\n",
      "\n",
      "    accuracy                           0.80       600\n",
      "   macro avg       0.80      0.80      0.80       600\n",
      "weighted avg       0.80      0.80      0.79       600\n",
      "\n",
      "### Spliting data...\n",
      "y_train: (2400,) y_val: (600,)\n",
      "### Preparing features...\n",
      "X_train: (2400, 300) X_val: (600, 300)\n",
      "### fitting...\n",
      " Classification Report on Train set \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       805\n",
      "           0       1.00      1.00      1.00       801\n",
      "           1       1.00      1.00      1.00       794\n",
      "\n",
      "    accuracy                           1.00      2400\n",
      "   macro avg       1.00      1.00      1.00      2400\n",
      "weighted avg       1.00      1.00      1.00      2400\n",
      "\n",
      " Classification Report on Val set \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.79      0.83      0.81       195\n",
      "           0       0.68      0.64      0.66       199\n",
      "           1       0.85      0.87      0.86       206\n",
      "\n",
      "    accuracy                           0.78       600\n",
      "   macro avg       0.78      0.78      0.78       600\n",
      "weighted avg       0.78      0.78      0.78       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_OF_VALIDATION = 3\n",
    "for _ in range(NUM_OF_VALIDATION):\n",
    "    print('### Spliting data...')\n",
    "    df_train, df_val = model_selection.train_test_split(\n",
    "        df_data, test_size=0.2, \n",
    "    #     random_state=42, \n",
    "        shuffle=True, \n",
    "    #     stratify=df_data['label_id']\n",
    "    )\n",
    "    y_train = df_train['label_id'].values\n",
    "    y_val = df_val['label_id'].values\n",
    "    print('y_train:', y_train.shape, 'y_val:', y_val.shape)\n",
    "\n",
    "    print('### Preparing features...')\n",
    "    X_train = combiner.fit_transform(df_train)\n",
    "    X_val = combiner.transform(df_val)\n",
    "    print('X_train:', X_train.shape, 'X_val:',  X_val.shape)\n",
    "    \n",
    "    print('### fitting...')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(' Classification Report on Train set ')\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    print(metrics.classification_report(y_true=y_train, y_pred=y_train_pred))\n",
    "    print(' Classification Report on Val set ')\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    print(metrics.classification_report(y_true=y_val, y_pred=y_val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_data, y_data)\n",
    "y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
