{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，\n",
    "# 如线条和物体局部，\n",
    "# 之后的最大池化层则用来降低卷积层对位置的敏感性。\n",
    "# 卷积层块由两个这样的基本单位重复堆叠构成。\n",
    "\n",
    "net = keras.Sequential([\n",
    "    # conv 1\n",
    "    keras.layers.Conv2D(\n",
    "        filters=6, kernel_size=5, activation='sigmoid', \n",
    "        input_shape=[28, 28, 1]\n",
    "    ), \n",
    "    keras.layers.MaxPool2D(pool_size=2, strides=2), \n",
    "    # conv 2\n",
    "    keras.layers.Conv2D(\n",
    "        filters=16, kernel_size=5, activation='sigmoid'\n",
    "    ), \n",
    "    keras.layers.MaxPool2D(pool_size=2, strides=2), \n",
    "    keras.layers.Flatten(), \n",
    "    keras.layers.Dense(120, activation='sigmoid'),\n",
    "    keras.layers.Dense(84, activation='sigmoid'),\n",
    "    keras.layers.Dense(10, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 16)          2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv2d output shape\t (1, 24, 24, 6)\n",
      "max_pooling2d output shape\t (1, 12, 12, 6)\n",
      "conv2d_1 output shape\t (1, 8, 8, 16)\n",
      "max_pooling2d_1 output shape\t (1, 4, 4, 16)\n",
      "flatten output shape\t (1, 256)\n",
      "dense output shape\t (1, 120)\n",
      "dense_1 output shape\t (1, 84)\n",
      "dense_2 output shape\t (1, 10)\n"
     ]
    }
   ],
   "source": [
    "X = tf.random.uniform([1, 28, 28, 1])\n",
    "for layer in net.layers:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = tf.reshape(X_train, list(X_train.shape) + [1])\n",
    "X_test = tf.reshape(X_test, list(X_test.shape) + [1])\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(\n",
    "    optimizer=keras.optimizers.SGD(\n",
    "        learning_rate=0.9, momentum=0.0, nesterov=False\n",
    "    ), \n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "54000/54000 [==============================] - 6s 111us/sample - loss: 0.4729 - accuracy: 0.8182 - val_loss: 0.4709 - val_accuracy: 0.8112\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 6s 109us/sample - loss: 0.4419 - accuracy: 0.8276 - val_loss: 0.4459 - val_accuracy: 0.8233\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 6s 109us/sample - loss: 0.4358 - accuracy: 0.8302 - val_loss: 0.4260 - val_accuracy: 0.8337\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 6s 113us/sample - loss: 0.4034 - accuracy: 0.8442 - val_loss: 0.4274 - val_accuracy: 0.8408\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 6s 112us/sample - loss: 0.4121 - accuracy: 0.8385 - val_loss: 0.4074 - val_accuracy: 0.8485\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 6s 115us/sample - loss: 0.4182 - accuracy: 0.8403 - val_loss: 0.4550 - val_accuracy: 0.8290\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 6s 110us/sample - loss: 0.4223 - accuracy: 0.8382 - val_loss: 0.3966 - val_accuracy: 0.8488\n",
      "Epoch 8/20\n",
      "54000/54000 [==============================] - 6s 110us/sample - loss: 0.3965 - accuracy: 0.8468 - val_loss: 0.4066 - val_accuracy: 0.8393\n",
      "Epoch 9/20\n",
      "54000/54000 [==============================] - 6s 107us/sample - loss: 0.4104 - accuracy: 0.8404 - val_loss: 0.4204 - val_accuracy: 0.8378\n",
      "Epoch 10/20\n",
      "54000/54000 [==============================] - 6s 106us/sample - loss: 0.3993 - accuracy: 0.8462 - val_loss: 0.3952 - val_accuracy: 0.8510\n",
      "Epoch 11/20\n",
      "54000/54000 [==============================] - 6s 106us/sample - loss: 0.3819 - accuracy: 0.8537 - val_loss: 0.4137 - val_accuracy: 0.8392\n",
      "Epoch 12/20\n",
      "54000/54000 [==============================] - 6s 105us/sample - loss: 0.3752 - accuracy: 0.8555 - val_loss: 0.4071 - val_accuracy: 0.8442\n",
      "Epoch 13/20\n",
      "54000/54000 [==============================] - 6s 106us/sample - loss: 0.3855 - accuracy: 0.8484 - val_loss: 0.3901 - val_accuracy: 0.8502\n",
      "Epoch 14/20\n",
      "54000/54000 [==============================] - 6s 105us/sample - loss: 0.3938 - accuracy: 0.8462 - val_loss: 0.4298 - val_accuracy: 0.8340\n",
      "Epoch 15/20\n",
      "54000/54000 [==============================] - 6s 105us/sample - loss: 0.3820 - accuracy: 0.8508 - val_loss: 0.4335 - val_accuracy: 0.8297\n",
      "Epoch 16/20\n",
      "54000/54000 [==============================] - 6s 107us/sample - loss: 0.3905 - accuracy: 0.8470 - val_loss: 0.4267 - val_accuracy: 0.8388\n",
      "Epoch 17/20\n",
      "54000/54000 [==============================] - 6s 105us/sample - loss: 0.3840 - accuracy: 0.8498 - val_loss: 0.3983 - val_accuracy: 0.8492\n",
      "Epoch 18/20\n",
      "54000/54000 [==============================] - 6s 106us/sample - loss: 0.3662 - accuracy: 0.8581 - val_loss: 0.3674 - val_accuracy: 0.8620\n",
      "Epoch 19/20\n",
      "54000/54000 [==============================] - 6s 111us/sample - loss: 0.3498 - accuracy: 0.8637 - val_loss: 0.4001 - val_accuracy: 0.8502\n",
      "Epoch 20/20\n",
      "54000/54000 [==============================] - 6s 114us/sample - loss: 0.3439 - accuracy: 0.8682 - val_loss: 0.3763 - val_accuracy: 0.8550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18b23722a08>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X_train, y_train, epochs=20, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 - 1s - loss: 0.3910 - accuracy: 0.8490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.39095582382678984, 0.849]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.evaluate(X_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
