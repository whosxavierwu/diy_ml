{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus_chars 54473\n",
      "idx2char 2581\n",
      "char2idx 2581\n",
      "vocab_size 2581\n",
      "corpus_indices 54473\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/jaychou_lyrics.txt') as fin:\n",
    "    corpus_chars = fin.read()\n",
    "    \n",
    "corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', '').replace(' ', '')\n",
    "print(\"corpus_chars\", len(corpus_chars))\n",
    "# corpus_chars = corpus_chars[:20000]\n",
    "# print(\"corpus_chars\", len(corpus_chars))\n",
    "\n",
    "idx2char = list(set(corpus_chars))\n",
    "print(\"idx2char\", len(idx2char))\n",
    "\n",
    "char2idx = {\n",
    "    ch: i\n",
    "    for i, ch in enumerate(idx2char)\n",
    "}\n",
    "print(\"char2idx\", len(char2idx))\n",
    "\n",
    "vocab_size = len(char2idx)\n",
    "print(\"vocab_size\", vocab_size)\n",
    "\n",
    "corpus_indices = [char2idx[ch] for ch in corpus_chars]\n",
    "print(\"corpus_indices\", len(corpus_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_random(corpus_indices, batch_size, num_steps):\n",
    "    # 在随机采样中，每个样本是原始序列上任意截取的一段序列。\n",
    "    # 相邻的两个随机小批量在原始序列上的位置不一定相毗邻。\n",
    "    # 因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。\n",
    "    # 在训练模型时，每次随机采样前都需要重新初始化隐藏状态。\n",
    "    corpus_size = len(corpus_indices)\n",
    "    # 减1是因为输出的索引是相应输入的索引加1\n",
    "    # An example contains num_steps elements of corpus\n",
    "    num_examples = (corpus_size - 1)  // num_steps\n",
    "    # A batch contains batch_size examples\n",
    "    num_batches = num_examples // batch_size\n",
    "    example_indices = list(range(num_examples))\n",
    "    random.shuffle(example_indices)\n",
    "    for batch_idx in range(num_batches):\n",
    "        X = []\n",
    "        Y = []\n",
    "        for example_idx in example_indices[batch_idx * batch_size: (batch_idx+1) * batch_size]:\n",
    "            X.append(corpus_indices[example_idx * num_steps: (example_idx+1) * num_steps])\n",
    "            Y.append(corpus_indices[example_idx * num_steps + 1: (example_idx+1) * num_steps + 1])\n",
    "        # output size : batch_size * num_steps\n",
    "        yield np.array(X), np.array(Y)\n",
    "\n",
    "        \n",
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, ctx=None):\n",
    "    # 可以令相邻的两个随机小批量在原始序列上的位置相毗邻。\n",
    "    # 这时候，我们就可以用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，\n",
    "    # 从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。\n",
    "    corpus_size = len(corpus_indices)\n",
    "    # A batch contains batch_size examples\n",
    "    num_steps_per_batch = corpus_size // batch_size  # ?\n",
    "    indices = np.array(corpus_indices[: batch_size*num_steps_per_batch]).reshape([batch_size, num_steps_per_batch])\n",
    "    num_batches = (num_steps_per_batch - 1) // num_steps\n",
    "    for batch_idx in range(num_batches):\n",
    "        X = indices[:, batch_idx * num_steps: (batch_idx+1) * num_steps]\n",
    "        Y = indices[:, batch_idx * num_steps + 1: (batch_idx+1) * num_steps + 1]\n",
    "        # output size : batch_size * num_steps\n",
    "        yield X, Y\n",
    "\n",
    "# my_seq = list(range(30))\n",
    "# print(\"data_iter_random\")\n",
    "# for X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n",
    "#     print('X: ', X, '\\nY:', Y, '\\n')\n",
    "\n",
    "# print(\"data_iter_consecutive\")\n",
    "# for X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\n",
    "#     print('X: ', X, '\\nY:', Y, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_params(num_inputs, num_hiddens, num_outputs):\n",
    "    W_xh = tf.Variable(\n",
    "        tf.random.normal(\n",
    "            shape=[num_inputs, num_hiddens], \n",
    "            mean=0, stddev=0.01, \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "    )\n",
    "    W_hh = tf.Variable(\n",
    "        tf.random.normal(\n",
    "            shape=[num_hiddens, num_hiddens], \n",
    "            mean=0, stddev=0.01, \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "    )\n",
    "    b_h = tf.Variable(\n",
    "        tf.zeros(num_hiddens), \n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    \n",
    "    W_hq = tf.Variable(\n",
    "        tf.random.normal(\n",
    "            shape=[num_hiddens, num_outputs], \n",
    "            mean=0, stddev=0.01, \n",
    "            dtype=tf.float32\n",
    "        )\n",
    "    )\n",
    "    b_q = tf.Variable(\n",
    "        tf.zeros(num_outputs), \n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    return [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "\n",
    "\n",
    "def rnn(Xs, H, params):\n",
    "    \"\"\"\n",
    "    :param Xs: num_steps * (batch_size, num_inputs)\n",
    "    :param H: (batch_size, num_hiddens)\n",
    "    :return \n",
    "        Ys: num_steps * (batch_size, num_outputs)\n",
    "        H: (batch_size, num_hiddens)\n",
    "    \"\"\"\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    Ys = []\n",
    "    for X in Xs:  # X: (batch_size, vocab_size)\n",
    "        H = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(H, W_hh) + b_h)\n",
    "        Y = tf.matmul(H, W_hq) + b_q  # (batch_size, num_outputs)\n",
    "        Ys.append(Y)\n",
    "    return Ys, H\n",
    "\n",
    "\n",
    "def predict_rnn(prefix, num_chars, rnn, params, num_hiddens, vocab_size, idx2char, char2idx):\n",
    "    state = tf.zeros([1, num_hiddens])\n",
    "    output = [\n",
    "        char2idx[prefix[0]]\n",
    "    ]\n",
    "    for t in range(num_chars + len(prefix) - 1):\n",
    "        X = tf.one_hot([output[-1]], vocab_size)  # (1, vocab_size)\n",
    "        Ys, state = rnn([X], state, params)\n",
    "        Y = Ys[0]\n",
    "        if t < len(prefix) - 1:\n",
    "            pred = char2idx[prefix[t+1]]\n",
    "        else:\n",
    "            pred = int(np.array(tf.argmax(Y, axis=1)))\n",
    "        output.append(pred)\n",
    "    return ''.join([idx2char[i] for i in output])\n",
    "\n",
    "\n",
    "def grad_clipping(grads, theta):\n",
    "    norm = 0.0\n",
    "    for i in range(len(grads)):\n",
    "        norm += tf.math.reduce_sum(grads[i] ** 2)\n",
    "    norm = np.sqrt(norm)\n",
    "    if norm <= theta:\n",
    "        return grads\n",
    "    return [grad * theta / norm for grad in grads]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20, perplexity 1551.870303\n",
      "分开\n",
      " - 分开的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "不分开\n",
      " - 不分开的的的的的的的的的的的的的的的的的的的的的的的的的的的的的的\n",
      "epoch 40, perplexity 655.144389\n",
      "分开\n",
      " - 分开大大大大大大大大大大大大大大大大大大大大大大大大大大大大大大\n",
      "不分开\n",
      " - 不分开大大大大大大大大大大大大大大大大大大大大大大大大大大大大大大\n",
      "epoch 60, perplexity 335.174087\n",
      "分开\n",
      " - 分开始烟一直一直一直一直一直一直一直一直一直一直一直一直一直一直\n",
      "不分开\n",
      " - 不分开始烟一直一直一直一直一直一直一直一直一直一直一直一直一直一直\n",
      "epoch 80, perplexity 252.423657\n",
      "分开\n",
      " - 分开始MMMMMMMMMMMMMMMMMMMMMMMMMMMMM\n",
      "不分开\n",
      " - 不分开始MMMMMMMMMMMMMMMMMMMMMMMMMMMMM\n",
      "epoch 100, perplexity 444.741905\n",
      "分开\n",
      " - 分开始终于她的爱情绪为什么会有一直一直一直一直一直一直一直一直一\n",
      "不分开\n",
      " - 不分开始终于她的爱情绪为什么会有一直一直一直一直一直一直一直一直一\n",
      "epoch 120, perplexity 1122.639064\n",
      "分开\n",
      " - 分开始终每一种每天每天每天每天每天每天每天每天每天每天每天每天每\n",
      "不分开\n",
      " - 不分开始终每一种每天每天每天每天每天每天每天每天每天每天每天每天每\n",
      "epoch 140, perplexity 7741.999924\n",
      "分开\n",
      " - 分开始移动的师读你的师读你的师读你的师读你的师读你的师读你的师读\n",
      "不分开\n",
      " - 不分开始移动的师读你的师读你的师读你的师读你的师读你的师读你的师读\n",
      "epoch 160, perplexity 2581.001324\n",
      "分开\n",
      " - 分开伽照奶針陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率\n",
      "不分开\n",
      " - 不分开伽照奶針陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率\n",
      "epoch 180, perplexity 2581.001324\n",
      "分开\n",
      " - 分开伽照奶針陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率\n",
      "不分开\n",
      " - 不分开伽照奶針陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率\n",
      "epoch 200, perplexity 2581.001324\n",
      "分开\n",
      " - 分开伽照奶針陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率\n",
      "不分开\n",
      " - 不分开伽照奶針陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率U尼b坟陰率\n"
     ]
    }
   ],
   "source": [
    "num_inputs = vocab_size\n",
    "num_hiddens = 256\n",
    "num_outputs = vocab_size\n",
    "\n",
    "num_steps = 10\n",
    "batch_size = 128\n",
    "prefixes = ['分开', '不分开']\n",
    "is_random_iter = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(10)\n",
    "\n",
    "data_iter_fn = data_iter_random if is_random_iter else data_iter_consecutive\n",
    "params = get_params(num_inputs, num_hiddens, num_outputs)\n",
    "for ep in range(200):\n",
    "    # 如使用相邻采样，在epoch开始时初始化隐藏状态\n",
    "    if not is_random_iter:\n",
    "        state = tf.zeros([batch_size, num_hiddens])\n",
    "    data_iter = data_iter_fn(corpus_indices, batch_size, num_steps)\n",
    "    loss_sum = 0.0\n",
    "    n = 0    \n",
    "    for X, Y in data_iter:  # X, Y: batch_size * num_steps\n",
    "        # 如使用随机采样，在每个小批量更新前初始化隐藏状态\n",
    "        if is_random_iter:\n",
    "            state = tf.zeros([batch_size, num_hiddens]) \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(params)\n",
    "            Xs = [  # num_steps * (batch_size, vocab_size)\n",
    "                tf.one_hot(x, vocab_size, dtype=tf.float32)\n",
    "                for x in X.T\n",
    "            ]\n",
    "            Ys, state = rnn(Xs, state, params)\n",
    "            Y_pred = tf.concat(Ys, axis=0)  # (num_steps*batch_size, num_outputs)\n",
    "            Y_true = tf.convert_to_tensor(\n",
    "                Y.T.reshape((-1, )), \n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            loss = tf.reduce_mean(\n",
    "                tf.losses.sparse_categorical_crossentropy(Y_true, Y_pred)\n",
    "            )\n",
    "        grads = tape.gradient(loss, params)\n",
    "        grads = grad_clipping(grads, 0.01)\n",
    "        optimizer.apply_gradients(zip(grads, params))\n",
    "        loss_sum += loss * len(Y_true)\n",
    "        n += len(Y_true)\n",
    "    if (ep + 1) % 20 == 0:\n",
    "        \"\"\"\n",
    "        困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，\n",
    "        最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；\n",
    "        最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；\n",
    "        基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。\n",
    "        显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小vocab_size。\n",
    "        \"\"\"\n",
    "        print('epoch %d, perplexity %f' % (ep + 1, math.exp(loss_sum / n)))\n",
    "        for prefix in prefixes:\n",
    "            print(prefix)\n",
    "            print(' -', predict_rnn(prefix, 30, rnn, params, num_hiddens, vocab_size, idx2char, char2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
