{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "for gpu in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间。\n",
    "# 对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。\n",
    "# 如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。\n",
    "\n",
    "# 使用批量归一化训练时，我们可以将批量大小设得大一点，从而使批量内样本的均值和方差的计算都较为准确。\n",
    "# 将训练好的模型用于预测时，单个样本的输出不应取决于批量归一化所需要的随机小批量中的均值和方差。\n",
    "# 一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(is_training, X, gamma, beta, running_mean, running_var, eps, momentum):\n",
    "    if not is_training:\n",
    "        X_hat = (X - running_mean) / np.sqrt(running_var + eps)\n",
    "    else:\n",
    "        if len(X.shape) == 2:  # dense layer\n",
    "            mean = np.mean(X, axis=0)\n",
    "            var = np.mean((X - mean) ** 2, axis=0)\n",
    "        else:  # conv\n",
    "            mean = np.mean(X, axis=(0, 2, 3), keepdims=True)\n",
    "            var = np.mean((X - mean) ** 2, axis=(0, 2, 3), keepdims=True)\n",
    "        X_hat = (X - mean) / np.sqrt(var + eps)\n",
    "        running_mean = momentum * running_mean + (1.0 - momentum) * mean\n",
    "        running_var = momentum * running_var + (1.0 - momentum) * var\n",
    "    Y = gamma * X_hat + beta\n",
    "    return Y, running_mean, running_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization(layers.Layer):\n",
    "    def __init__(self, decay=0.9, epsilon=1e-5, **kwargs):\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "        super().__init__(**kwargs)\n",
    "        return \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(\n",
    "            name='gamma', \n",
    "            shape=[input_shape[-1]], \n",
    "            initializer=tf.initializers.ones, \n",
    "            trainable=True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name='beta', \n",
    "            shape=[input_shape[-1]], \n",
    "            initializer=tf.initializers.zeros, \n",
    "            trainable=True\n",
    "        )\n",
    "        self.running_mean = self.add_weight(\n",
    "            name='running_mean', \n",
    "            shape=[input_shape[-1]], \n",
    "            initializer=tf.initializers.zeros, \n",
    "            trainable=False\n",
    "        )\n",
    "        self.running_var = self.add_weight(\n",
    "            name='running_var', \n",
    "            shape=[input_shape[-1]], \n",
    "            initializer=tf.initializers.ones, \n",
    "            trainable=False\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "        return \n",
    "    \n",
    "    def assign_running_mean(self, variable, value):\n",
    "        delta = variable * self.decay + value * (1 - self.decay)\n",
    "        return variable.assign(delta)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inputs, training):\n",
    "        if not training:\n",
    "            mean = self.running_mean\n",
    "            var = self.running_var\n",
    "        else:\n",
    "            batch_mean, batch_var = tf.nn.moments(inputs, list(range(len(inputs.shape) - 1)))\n",
    "            mean_update = self.assign_running_mean(self.running_mean, batch_mean)\n",
    "            var_udpate = self.assign_running_mean(self.running_var, batch_var)\n",
    "            self.add_update(mean_update)\n",
    "            self.add_update(var_udpate)\n",
    "            mean = batch_mean\n",
    "            var = batch_var\n",
    "        output = tf.nn.batch_normalization(\n",
    "            inputs, \n",
    "            mean=mean, \n",
    "            variance=var, \n",
    "            offset=self.beta, \n",
    "            scale=self.gamma, \n",
    "            variance_epsilon=self.epsilon\n",
    "        )\n",
    "        return output\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = keras.Sequential([\n",
    "    layers.Conv2D(6, kernel_size=5), \n",
    "    BatchNormalization(), \n",
    "    layers.Activation('sigmoid'), \n",
    "    layers.MaxPool2D(pool_size=2, strides=2), \n",
    "    \n",
    "    layers.Conv2D(16, kernel_size=5), \n",
    "    BatchNormalization(), \n",
    "    layers.Activation('sigmoid'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2), \n",
    "    \n",
    "    layers.Flatten(), \n",
    "    \n",
    "    layers.Dense(120), \n",
    "    BatchNormalization(), \n",
    "    layers.Activation('sigmoid'), \n",
    "    \n",
    "    layers.Dense(84), \n",
    "    BatchNormalization(), \n",
    "    layers.Activation('sigmoid'),\n",
    "    \n",
    "    layers.Dense(10, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 8s 165us/sample - loss: 0.4083 - accuracy: 0.9383 - val_loss: 0.1729 - val_accuracy: 0.9512\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 5s 98us/sample - loss: 0.0854 - accuracy: 0.9766 - val_loss: 0.0692 - val_accuracy: 0.9784\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0630 - accuracy: 0.9812 - val_loss: 0.0682 - val_accuracy: 0.9793\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0526 - accuracy: 0.9840 - val_loss: 0.0711 - val_accuracy: 0.9769\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0452 - accuracy: 0.9859 - val_loss: 0.1625 - val_accuracy: 0.9504\n",
      "10000/10000 - 1s - loss: 0.1524 - accuracy: 0.9511\n",
      "Test loss: 0.15243182272315026\n",
      "Test accuracy: 0.9511\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "net.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "history = net.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=5,\n",
    "                    validation_split=0.2)\n",
    "test_scores = net.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'sequential_1/batch_normalization_4/gamma:0' shape=(6,) dtype=float32, numpy=\n",
       " array([1.0482427 , 1.0853697 , 1.1533467 , 0.89600337, 1.1308107 ,\n",
       "        0.9885612 ], dtype=float32)>,\n",
       " <tf.Variable 'sequential_1/batch_normalization_4/beta:0' shape=(6,) dtype=float32, numpy=\n",
       " array([-0.49997556,  0.38060945, -0.79884404, -0.21281163,  0.23447165,\n",
       "        -0.02724187], dtype=float32)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.get_layer(index=1).gamma,net.get_layer(index=1).beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = keras.Sequential([\n",
    "    layers.Conv2D(6, kernel_size=5), \n",
    "    layers.BatchNormalization(), \n",
    "    layers.Activation('sigmoid'), \n",
    "    layers.MaxPool2D(pool_size=2, strides=2), \n",
    "    \n",
    "    layers.Conv2D(16, kernel_size=5), \n",
    "    layers.BatchNormalization(), \n",
    "    layers.Activation('sigmoid'),\n",
    "    layers.MaxPool2D(pool_size=2, strides=2), \n",
    "    \n",
    "    layers.Flatten(), \n",
    "    \n",
    "    layers.Dense(120), \n",
    "    layers.BatchNormalization(), \n",
    "    layers.Activation('sigmoid'), \n",
    "    \n",
    "    layers.Dense(84), \n",
    "    layers.BatchNormalization(), \n",
    "    layers.Activation('sigmoid'),\n",
    "    \n",
    "    layers.Dense(10, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/5\n",
      "48000/48000 [==============================] - 6s 122us/sample - loss: 0.4251 - accuracy: 0.9371 - val_loss: 0.1287 - val_accuracy: 0.9665\n",
      "Epoch 2/5\n",
      "48000/48000 [==============================] - 4s 90us/sample - loss: 0.0909 - accuracy: 0.9739 - val_loss: 0.6474 - val_accuracy: 0.7592\n",
      "Epoch 3/5\n",
      "48000/48000 [==============================] - 4s 91us/sample - loss: 0.0672 - accuracy: 0.9794 - val_loss: 0.1292 - val_accuracy: 0.9602\n",
      "Epoch 4/5\n",
      "48000/48000 [==============================] - 4s 90us/sample - loss: 0.0551 - accuracy: 0.9832 - val_loss: 0.4795 - val_accuracy: 0.8296\n",
      "Epoch 5/5\n",
      "48000/48000 [==============================] - 4s 89us/sample - loss: 0.0458 - accuracy: 0.9855 - val_loss: 0.1831 - val_accuracy: 0.9448\n",
      "10000/10000 - 1s - loss: 0.1705 - accuracy: 0.9503\n",
      "Test loss: 0.17049654459506272\n",
      "Test accuracy: 0.9503\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "x_test = x_test.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "net.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "history = net.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=5,\n",
    "                    validation_split=0.2)\n",
    "test_scores = net.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss:', test_scores[0])\n",
    "print('Test accuracy:', test_scores[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
